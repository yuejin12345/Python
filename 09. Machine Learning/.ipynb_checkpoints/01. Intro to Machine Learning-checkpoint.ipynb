{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Machine Learning\n",
    "\n",
    "### Objective\n",
    "\n",
    "* Use the pandas-profiling package to do EDA\n",
    "* Know that you cannot do machine learning in scikit-learn with missing data\n",
    "* Understand basic strategies for filling missing data\n",
    "* Fill in missing data\n",
    "* Extract data into NumPy arrays\n",
    "* Learn the three step process for doing machine learning in Scikit-Learn\n",
    "    1. Import the model\n",
    "    2. Instantiate the model\n",
    "    3. Train the model\n",
    "* Input data must be 2D array\n",
    "* Make predictions\n",
    "* Measure performance by calculating accuracy\n",
    "\n",
    "## Typical Workflow for Beginners\n",
    "* Find dataset\n",
    "    * [Kaggle Datasets][1]\n",
    "    * [data.world][2]\n",
    "    * [data.gov][3]\n",
    "    * [UCI Machine Learning Repository][10]\n",
    "* Read data into Pandas\n",
    "* Clean data\n",
    "* Exploratory data analysis with basic statistics and visualizations\n",
    "* Define Problem\n",
    "* Train and Evaluate model with Scikit-Learn\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [Hands on Machine Learning with Scikit-Learn and Tensor Flow][6], very popular book\n",
    "* [Introduction to Statistical Learning][8] by Trevor Hastie and Robert Tibshirani\n",
    "* My Solutions to [Introduction to Statistical Learning][11] using Python\n",
    "* Full college class on [Applied Machine Learning][7] by Andreas Mueller, core contributor to Scikit-Learn \n",
    "* Tutorial in [Jupyter Notebooks][5] from Andreas Mueller\n",
    "* My article on the [new workflow from Pandas to Scikit-Learn][9]\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets\n",
    "[2]: https://data.world/\n",
    "[3]: https://www.data.gov/\n",
    "[5]: https://github.com/amueller/scipy-2016-sklearn\n",
    "[6]: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291\n",
    "[7]: http://www.cs.columbia.edu/~amueller/comsw4995s18/schedule/\n",
    "[8]: http://www-bcf.usc.edu/~gareth/ISL/data.html\n",
    "[9]: https://medium.com/dunder-data/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62\n",
    "[10]: https://archive.ics.uci.edu/ml/index.php\n",
    "[11]: https://github.com/tdpetrou/Machine-Learning-Books-With-Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Dataset\n",
    "\n",
    "We will be use the [heart disease][1] dataset from the ISLR book. Let's read it in and take a peak at the data.\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv('../data/heart.csv')\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the columns with help from the data dictionary\n",
    "Always find or create a data dictionary when beginning a project. In this instance, it is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv('../data/heart_data_dictionary.csv')\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the data types and ensure they match the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipping EDA\n",
    "Typically, you should complete a through exploratory data analysis before commencing machine learning. For these introductory notebooks, we will skip this step and jump right into machine learning. This allows us to focus on the mechanics of machine learning.\n",
    "\n",
    "### Use Pandas Profiling instead\n",
    "As a drop-in replacement for manual EDA, we can use the [pandas-profiling package][1]. Install it from the command line with `conda install pandas-profiling`. It has only one major object, `ProfileReport`. Simply pass the DataFrame to it and it will provide you with lots of basic descriptions of the dataset.\n",
    "\n",
    "[1]: https://github.com/pandas-profiling/pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Currently there are a lot of warnings that pandas-profiling emits when imported.\n",
    "# Ignore them with the following\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas_profiling as pf\n",
    "pf.ProfileReport(heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the type of machine learning problem - Supervised or Unsupervised\n",
    "Before beginning, we need to identify the type of machine learning problem we have. In this problem, we are predicting whether or not someone has heart disease and therefore are doing supervised learning.\n",
    "\n",
    "\n",
    "## Identifying the target variable\n",
    "We are interested in predicting whether some one has heart disease or not, which is the `disease` column in our DataFrame. \n",
    "\n",
    "\n",
    "## Classification or Regression\n",
    "There are two classes of disease (0 or 1), which we see from the data dictionary, correspond to either no or yes. Thus, we have a **classification** problem. From the profile report, we see that 46% of the observations do have heart disease and 54% do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum data preparation\n",
    "It is common to do lots of data preparation, but in this notebook, we only do the minimum necessary to enable scikit-learn models to work for us. \n",
    "\n",
    "### Check for missing values\n",
    "Scikit-learn does not allow for any missing values. Let's check for them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must fill missing values\n",
    "We have a couple columns that are missing values. The simplest thing we can do to resolve this issue is to drop the rows (or columns) containing the missing values. Calling `heart.dropna()` will drop every row with a missing value in it. Otherwise, we need to fill them with some value.\n",
    "\n",
    "### Strategies for filling missing values\n",
    "There are a number of strategies that have been developed to fill in missing values. This notebook focuses on the simplest strategies. The strategy used to fill in a missing value depends on the type of data in the column.\n",
    "\n",
    "#### Filling in missing values for categorical columns\n",
    "A common strategy is to use the **most frequent** value for categorical columns. \n",
    "\n",
    "A different strategy is to randomly select one of the non-missing values in the column. This preserves the distribution of the values in that column and is sometimes called **hot-deck** imputation.\n",
    "\n",
    "#### Issues with these strategies\n",
    "Filling missing values with the most frequent value for that column might bias our results significantly. If the most frequent value is only slightly more frequent than the second most frequent value, then this value can be significantly overrepresented.\n",
    "\n",
    "#### Filling in missing values for continuous columns\n",
    "Continuous data allows for other strategies with the simplest being using the mean or median. Hot-deck imputation can work as well.\n",
    "\n",
    "#### More advanced strategies\n",
    "There are many more advanced strategies that have been developed with most of them relying on using machine learning to fill in the missing values. These will be discussed in a different notebook.\n",
    "\n",
    "### Filling the missing values in the heart dataset - know the type of variable \n",
    "In this dataset, the column `ca` has numeric values but is actually a categorical (it represents the number of major vessels colored by flourosopy) . It would not make any sense to use the mean to fill in the value here. Let's use `value_counts` to find the most common number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_ca = heart['ca'].value_counts()\n",
    "vc_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_fill = vc_ca.index[0]\n",
    "ca_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for `thal`\n",
    "The `thal` column is also categorical, so we can again compute the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_thal = heart['thal'].value_counts()\n",
    "vc_thal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thal_fill = vc_thal.index[0]\n",
    "thal_fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the missing values with `fillna`\n",
    "Pass a dictionary to the `fillna` method mapping the column name to the value you would like to fill it with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = heart.fillna({'ca': ca_fill, 'thal': thal_fill})\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up: Change data type of `ca` to int\n",
    "Because there was missing data in the `ca` column, its data type was float. We can now change it to an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current data type\n",
    "heart['ca'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to int and check\n",
    "heart['ca'] = heart['ca'].astype('int')\n",
    "heart['ca'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data into NumPy arrays\n",
    "Scikit-learn was built to integrate directly with NumPy and has traditionally (until version 0.20) had weak integration with Pandas. \n",
    "\n",
    "For now, all data will be taken out of Pandas DataFrames and put into NumPy arrays. By convention (as done in the Scikit-Learn documentation), use **`X`** and **`y`** as Python variable names for the arrays. Use the **`values`** DataFrame/Series attribute to retrieve the underlying NumPy arrays.\n",
    "\n",
    "## Begin by using a single feature\n",
    "It is possible to do machine learning with every single feature in the model, but when first beginning, it is good to keep things simple and use a single feature. \n",
    "\n",
    "## Must use a numeric column!\n",
    "You cannot do machine learning directly in Scikit-Learn with string columns. You can only use numeric columns without any missing values. In order to use string columns, you must encode the strings as numeric values (more on this in later notebooks).\n",
    "\n",
    "## Use `max_hr` column as feature\n",
    "We pick `max_hr`, which from the profile report was the only variable that was negatively correlated to heart disease.\n",
    "\n",
    "### Extract data into NumPy arrays with the `values` attribute\n",
    "We extract the data from Pandas to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart['max_hr'].values\n",
    "y = heart['disease'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify we are in NumPy and output a few of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready for machine learning in 3 steps\n",
    "All machine learning models in scikit-learn use the same three-step process to train.\n",
    "\n",
    "1. Import the model\n",
    "2. Instantiate the model\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the model from Scikit-Learn\n",
    "The scikit-learn library is structured differently than Pandas. It keeps all of its functionality tucked away in separate modules. By convention, we directly import the object we want by referencing the module where it is located. In this case, we will import one of the simplest classification models - Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. Import the model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait, why is the word regression in the name? Isn't this classification?\n",
    "Unfortunately, the name \"Logistic Regression\" is very confusing. Despite having the word \"regression\" in the name, it is used for classification and not regression. The logistic regression model does return a continuous value, but it is always a number between 0 and 1 which represents the **probability** of each observation being classified as one class or another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Instantiate the model (estimator)\n",
    "In step 1, when we import a model, we have been handed a blueprint. It is not built and not ready to use. We must instantiate it (create an instance of it) in order to actually use it. Scikit-Learn uses the term **estimator** to refer to each model. You can also use the phrase **instantiate the estimator**.\n",
    "\n",
    "### \"Constructing our machine learning vehicle\"\n",
    "An additional phrase I like to use for this step is **constructing our machine learning vehicle** which really emphasizes what is happening. Below, the variable `logr` is being assigned the result of our machine learning vehicle construction. It is our physical object that will do the machine learning and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insantiating with default values\n",
    "Go back into the last code cell and press **shift + tab + tab** with your cursor inside the parentheses. Notice all the parameters and their default values. When we constructed our model, we used these default values to build it. All these parameters are called **hyperparameters** in machine learning. These are 'specifications' to which our model was built. We can change them or **tune** them to construct our model in a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the model\n",
    "To train the model, we must give it some data. Our data is stored in the `X` and `y` Numpy arrays. All scikit-learn estimators use the **`fit`** method to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very annoying Gotcha!\n",
    "Scikit-Learn forces you to use a 2-dimensional array for your input values. When we selected one column above as our **`X`** array, it was a single dimension. Verify this with the `ndim` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the help from the error message\n",
    "The error message gives us explicit advice on how to transform our input data. We need to call the `reshape` method. It will transform the data from a single dimensional array with 303 values into a two dimensional array with 303 rows and 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1)\n",
    "X.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (again): Train the model\n",
    "Now that we have two-dimensional data for our input, scikit-learn will no longer complain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model is trained. What does that mean?\n",
    "All machine learning models have different objectives that must be met in order for them to be trained. For instance, with logistic regression, the objective is to set the parameters of the model in such a manner that it predicts heart disease with 100% accuracy.\n",
    "\n",
    "### Training is an iterative process\n",
    "During training, scikit-learn slowly changes the values of the model parameters in order to get the highest possible accuracy. This is an **iterative process** done using a **for** loop. Advanced numerical analysis is used to determine how to change the parameters during each iteration. \n",
    "\n",
    "Since, it is unlikely that the model can achieve 100% accuracy, there is a stopping criterion that gets triggered whenever the accuracy fails to improve by a certain amount. This iterative process takes place during the execution of the `fit` method.\n",
    "\n",
    "## Make a prediction\n",
    "All supervised learning estimators in scikit-learn have a **`predict`** method. Let's use it to make some predictions about heart disease.\n",
    "\n",
    "### Use a `max_hr` value that is within the range found in the dataset\n",
    "It only makes sense to predict with a value that we have seen in the dataset. Let's find the minimum and maximum value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.min(), X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the gotcha\n",
    "We have to pass the `predict` method a 2D array, just like we did when we trained it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([100, 150, 200])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the same trick to reshape a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape(-1, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the prediction\n",
    "Call the `predict` method with the NumPy array to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.predict(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of prediction\n",
    "The returned array holds the classes that our model predicts. In this case it predicts heart disease for the first input (a `max_hr` of 100), and no heart disease for the last two (`max_hr` of 150 and 200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction for all inputs\n",
    "We can make predictions on all of the inputs by passing the `predict` method our original input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure performance by calculating the accuracy\n",
    "That's great that we have a prediction, but we need to measure its performance. To do so, we must have the true outcome of each patient. Using the above data, the outcome is stored in the `y` variable. The simplest way to measure performance for a classification problem is to calculate the **accuracy** - which is defined as the percentage of the predictions that are correct. \n",
    "\n",
    "In this case, we are correct when the model predicts 0 when the true outcome is 0 or when the model predicts 1 and the true outcome is 1.\n",
    "\n",
    "## Use the `score` method to calculate accuracy\n",
    "All supervised learning estimators have a `score` method. The `score` method makes a prediction and calculates the accuracy. Pass it the input data and the expected output. We achieve 67% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "<span  style=\"color:green; font-size:16px\">Select a different variable besides `max_hr` and repeat the three step process to train a single-feature logistic regression model. Keep trying other numeric columns. Can you beat 67% accuracy? Can you define a function that automates this process?  Have the function accept the string name of the column to train on and return the accuracy.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
