{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating Model Performance\n",
    "\n",
    "### Objectives\n",
    "* Learn how to estimate a model's performance on unseen data\n",
    "* Understand why evaluating on the training set is wrong\n",
    "* Know what generalization performance is\n",
    "* Learn how to create a training/testing split\n",
    "\n",
    "### Resources\n",
    "* [Four-part series][1] on model evaluation by Sebastian Raschka\n",
    "* Scikit-Learn user guide on [cross validation][2]\n",
    "\n",
    "## Introduction\n",
    "In the previous notebooks, we have been evaluating our model performance by calling each estimator's `score` method which returned the accuracy. All of this scoring we've done so far is wrong. Evaluating our model's performance on the same data that we trained them on, will not give us a true measure of how likely it is to perform in the future.\n",
    "\n",
    "This is akin to taking a test in school where the professor hands you the questions and answers beforehand. Our score gives us very little information about how well we would do on questions we've never encountered before.\n",
    "\n",
    "## Goal: Estimate Accuracy on Future Unseen Data\n",
    "One of the major goals of machine learning is to have a good idea of how well the model will perform on future unseen data. When the model is released into the wild, how will it perform? This is sometimes referred to as **generalization error** (or [generalization error][3]). In other words, it is a measurement of how well the model generalizes to data that it has not seen before. There are several ideas that have been developed to calculate this generalization performance.\n",
    "\n",
    "# First idea, create a \"holdout\" test set that is not used during training\n",
    "A simple idea is to partition the original dataset into two distinct datasets, one to be used during training, and another to be withheld for evaluating performance. This holdout dataset is also referred to as the **test** dataset.\n",
    "\n",
    "### Use the `train_test_split` helper function\n",
    "The helper function `train_test_split` may be used to split the data into training and test sets. It accepts both the input, `X`, and labels, `y`, and splits each one into train and test sets returning four total arrays. Typical splits place more data in the training than the test set. By default, a 75/25 split is used, but we can use the `test_size` parameter to change this. Below, we choose two columns to be in our input dataset and make a 70/30 split.\n",
    "\n",
    "[1]: https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html\n",
    "[2]: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "[3]: https://en.wikipedia.org/wiki/Generalization_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "heart = pd.read_csv('../data/heart.csv')\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart[['max_hr', 'rest_bp']].values\n",
    "y = heart['disease'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model just on the training data\n",
    "To get an accurate measure of future performance, the model must never be exposed to the data in the test set. Below, we build a decision tree classifier and train on just the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating (incorrectly) on the training data\n",
    "Before we evaluate on the test data, let's repeat our mistakes from the previous notebooks and score ourselves on data that the model has already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test data\n",
    "Now that our model has not touched the test data, we can safely assess its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance is worse than the baseline\n",
    "The results from the test performance indicate we are slightly worse off than where we started. We have built a truly atrocious model unable to beat the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Idea: Cross Validation\n",
    "\n",
    "One issue with the first idea above is that that there is only a single test set to evaluate our results on. If this particular test set, by chance, was not a good representation of the data it might not accurately measure our generalization performance. \n",
    "\n",
    "**Cross Validation** is a set of ideas that improves upon evaluation based on a single test set. Instead, we can make many repetitive train/test splits of our data and record a score for each split. In this manner, we can calculate multiple performance scores, giving us more feedback on what to expect from unseen data.\n",
    "\n",
    "## Many flavors of cross validation\n",
    "There are multiple strategies of splitting the data that fall under the umbrella of cross validation. By far the most common form of cross validation is **K-fold cross validation**. In this method, the data is split into k distinct partitions, where k is usually between 5 and 10. One of these k partitions is used as the test set, while the other k-1 partitions are used for training. \n",
    "\n",
    "After the first round of testing, a different partition is used for testing and the other k-1 partitions are again used or training. The model is refit to the new training data and evaluated on the new test data. After all k partitions have been used for testing, the procedure ends.\n",
    "\n",
    "The result is a total of k scores, which can then be averaged to yield a better overall performance metric. Take a look at the image below to see how the data would be split when doing a 5-fold cross validation.\n",
    "\n",
    "![][1]\n",
    "\n",
    "## K-Fold Cross Validation in Scikit-Learn\n",
    "\n",
    "The **`cross_val_score`** function automates the process of doing cross validation for us in a single line. Pass it an estimator, the data, and the number of validation sets to use. The estimator does NOT need to be trained ahead of time. Below, we use 5 folds (splits) which places 80% of the data as training during each round and 20% as testing.\n",
    "\n",
    "[1]: images/kf.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logr, X, y, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An array of scores is returned\n",
    "An array of k scores is returned. We can take the mean and standard deviation to get a summary of our results. Notice there is quite a large amount of variance in the predictions, ranging from 60% to over 80%. This is likely due to the small number of samples in each test set. Since there are only 303 total observations there are only going to be 60 in each test set. A larger dataset would likely yield smaller variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No model is returned\n",
    "When using the `cross_val_score` function for k-fold cross validation, k models are trained, but none of those fitted models are returned. Only the scores are returned.\n",
    "\n",
    "## Cross validation evaluates performance. Us all the data to build the final model\n",
    "Cross validation is a procedure to evaluate the model performance, it does not return us a fitted model. As the final model to use in the future, you would train it on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with all of the data\n",
    "logr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting our generalization performance estimate\n",
    "It's important to summarize the information produced from cross validation. We would say that, our logistic regression instance, `logr`, trained with `max_hr` and `rest_bp` variables can expect to achieve 69% accuracy with a standard deviation of 7.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other flavors of cross validation\n",
    "There are several other [cross validation strategies][1] that scikit-learn offers and refers to them as [splitter classes][2]. Please click the previous link to see all of the possibilities.\n",
    "\n",
    "### Splitter classes are not estimators\n",
    "Most of the Scikit-Learn API can be divided up into estimators and helper functions. The splitter classes are a special case that don't fall into one of those categories. These objects need to be instantiated but they are not machine learning vehicles, and instead are used to do cross validation.\n",
    "\n",
    "## The `KFold` splitter\n",
    "When we first called `cross_val_score`, we passed the `cv` parameter an integer, which was used to determine the number of splits. Instead of passing it an integer, we can pass it an instance of one of the splitter classes. By default, `cross_val_score` does a stratified k-fold cross validation, which will be discussed down below. Instead, we can specify the type of cross validation exactly with the`KFold` splitter class. Here, we import it and instantiate it ready to do 5 splits.\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators\n",
    "[2]: https://scikit-learn.org/stable/modules/classes.html#splitter-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No cross validation has happened at this point\n",
    "`kf` is a simple object and is simply waiting to be used during cross validation, such as with `cross_val_score`. Let's pass it to the `cv` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logr, X, y, cv=kf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold cross validation\n",
    "Stratified k-fold cross validation will make k partitions as our k-fold did above, except that it will preserve the distribution of classes in each fold. For instance, in this dataset 46% of people have heart disease. Stratified k-fold will ensure that each fold has this same percentage of people with heart disease. Stratification is more important for heavily imbalanced datasets - those which have low-frequency classes.\n",
    "\n",
    "Let's import and use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logr, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will be the exact same as the original\n",
    "The default cross validation strategy is stratified k-fold, so it produces the same results from the very first execution of this method with `cv=5`. We again verify this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data\n",
    "By default, scikit-learn splits the dataset from the beginning in the order that it was received. If the dataset is ordered in some way, this might affect the results. The model might not have seen any observations from a particular segment of the data and therefore be unable to train on it.\n",
    "\n",
    "To handle this issue, use the `shuffle` parameter available in many of the splitter classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cross_val_score(logr, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Practice using the `ShuffleSplit`, `RepeatedKFold`, and `LeavePOut` splitters. Read the documentation and find out how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
