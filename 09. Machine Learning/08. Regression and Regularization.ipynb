{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Regression\n",
    "Looks now look at the housing dataset which has a continuous value output. There are 80 features in the model. Read more about them at the [online data dictionary][1].\n",
    "\n",
    "[1]: https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/data_description.txt?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1537163162&Signature=DJyH9SDDyhtHiKfFYNG%2F3sBpCXzB1BXkTCh9aakN3neRlcl%2BCalteIiJH9Kni%2FeGpAhdN6xQHgXinxPAJwNbG1zCymFsdG0%2FKtDBGHsthihgADyhpIkhRLhUjiwDPqstLQFe3tiAoGXc4EtBI1REpy5Az1kK4ah1X7ccxAHsluhiVloH9mteIMiYNsUkE%2BB1Gt3OnJQOYFAtRAcAyUFwf3%2BoQIrmTmF8mety89WSsi8qwdBqnGgea8eCLFba0akRrDJg9cvwtI%2F%2FFCScjQZ16l%2FjYF24ZRDtMWXW0CnXz2Q4%2Fr2Zo3rJijgr7L0b%2BlmuyordaUWEr%2BA3UrjNI3WL4g%3D%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('../data/housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify target variable and type of problem\n",
    "The target variable is sale price which is a continuous value therefore making this a regression problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose one predictor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing['GrLivArea'].values.reshape(-1, 1)\n",
    "y = housing['SalePrice'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the simplest regression model you can think of?\n",
    "### Exercise - build a model in scikit-learn that implement's this simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Outside of our dummy estimator, linear regression is among the simplest regression model we can choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lr = LinearRegression()\n",
    "cross_val_score(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective for regression - Minimize squared distance between actual and predicted values (R-squared)\n",
    "\n",
    "![][1]\n",
    "\n",
    "[1]: images/r2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Regression in Scikit-Learn\n",
    "Find several more regression models in the API, import, instantiate, fit, and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfiting\n",
    "\n",
    "![][1]\n",
    "\n",
    "Credit: Andrew Ng ML class\n",
    "\n",
    "[1]: images/reg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible vs Inflexible models\n",
    "Given an infinite amount of flexibility, a model can be built to perfectly fit every possible point in the training dataset.\n",
    "\n",
    "### More on flexibility\n",
    "Flexibility can be achieved in two main ways:\n",
    "* Using more parameters in the model\n",
    "* Using a more flexible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is called the Bias/Variance Tradeoff\n",
    "\n",
    "### Definitions\n",
    "* **Bias** - You have lots confidence in your assumptions about the model and use a model that is not flexible enough to capture more complex relationships\n",
    "* **Variance** - You put too much reliance on the data. You build a model that is very sensitive to the inputs and overfits the data. Small changes of the input data can lead to drastically different fitted values of the parameters.\n",
    "\n",
    "High variance is often referred to as **memorization**.\n",
    "![](images/bv_trade.png?1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with all numeric variables\n",
    "Let's build a model with the numeric variables. Notice, we dropped the final sale price since that is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id is an integer label for the rows\n",
    "# MSSubClass is a neighborhood\n",
    "sale_price = housing['SalePrice']\n",
    "housing_num = housing.select_dtypes('number').drop(columns=['Id', 'MSSubClass', 'SalePrice'])\n",
    "housing_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_num_filled = housing_num.fillna(housing_num.median())\n",
    "housing_num_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_num_filled.values\n",
    "y = sale_price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)\n",
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not much overfitting\n",
    "Our generalization performance was only about 3% less than the cross validated performance giving us confidence that it is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "In machine learning, regularization is a process of constraining our model during training so that it doesn't overfit to the training data. We purposefully weaken the model so that it can't fit the noise. It is, of course, possible to apply too much of a dampening affect so that the signal is lost. The key is to find just the right amount regularization.\n",
    "\n",
    "## Regularization in linear regression\n",
    "Without regularization, the goal of linear regression is to get our predictions as close to the actual points as possible. However, when using regularization, we have an additional goal of keeping the values of the fitted parameters within a certain threshold.\n",
    "\n",
    "\n",
    "# Ridge/Lasso Regression - A new problem formulation\n",
    "* Minimize the sum of squared errors.\n",
    "* For ridge, the sum of the squared values of the parameters must be less than some number (Scikit-Learn uses **`C`**)\n",
    "* For lasso, we use the sum of the absolute values of the parameters.\n",
    "\n",
    "\n",
    "## There is a boundary for your parameters - Find the minimum sum of squares within this boundary\n",
    "\n",
    "![][1]\n",
    "\n",
    "\n",
    "[1]: images/ridge.png?1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can no longer reach the minimum squared error\n",
    "The center of the red contours represents the optimal values of the parameters without constraints. \n",
    "\n",
    "The points where the contour touches the blue-green region is the lasso/ridge solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Ridge and Lasso regressors in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you find the best alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization works best when the features are of the same scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale features so they are all in the same range\n",
    "Standardization is the process of transforming a feature such that it has mean 0 and standard deviation of 1. Scikit-Learn provides the **`StandardScaler`** estimator. This object is the first **transformer** we have seem, which is similar to an estimator, but literally transforms the data into something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = ss.fit_transform(X)\n",
    "X_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try again with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_scaled, y)\n",
    "ridge.score(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(ridge, X_scaled, y=y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_scaled, y).score(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.fit(X_scaled, y).score(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not enough regularization to see a difference. Let's apply more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "<span  style=\"color:green; font-size:16px\">Look through the methods of the StandardScaler instance.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "<span  style=\"color:green; font-size:16px\">Can you verify that the coefficients are shrinking with increasing more regularization? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
