{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inspecting the Estimator\n",
    "\n",
    "### Objectives\n",
    "* Inspect the estimator after it has been trained\n",
    "* Find the parameter values of the model\n",
    "* Manually calculate the probabilities by defining a custom function\n",
    "* Plot the estimated probability curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo our logistic regression output\n",
    "Let's rerun the same commands to build a logistic regression model with the heart disease dataset, using `max_hr` as the predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv('../data/heart.csv')\n",
    "ca_fill = heart['ca'].value_counts().index[0]\n",
    "thal_fill = heart['thal'].value_counts().index[0]\n",
    "heart = heart.fillna({'ca': ca_fill, 'thal': thal_fill})\n",
    "\n",
    "X = heart['max_hr'].values.reshape(-1, 1)\n",
    "y = heart['disease'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes and methods of our Estimator\n",
    "Our estimator object, `logr` in this case, has some interesting attributes and methods worth exploring.\n",
    "\n",
    "### Return all the values of the hyperparameters\n",
    "When we instantiated our model, we set all the model hyperparameters with the default values. We can retrieve them with the `get_params` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of these are available directly as attributes\n",
    "You can directly access these hyperparameters as attributes from `logr`. Let's see a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.fit_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the classes\n",
    "Another useful attribute is `classes_` which hold the unique class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction for all possible values\n",
    "To help get a better sense of how `max_hr` relates to heart disease, we can make a prediction for every (integer) value between the minimum and maximum `max_hr`. The NumPy `arange` function creates evenly spaced one-dimensional arrays from given start, stop, and step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.min(), X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.arange(X.min(), X.max()).reshape(-1, 1)\n",
    "inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression returns a probability\n",
    "Although our logistic regression model returned 0's and 1's above, it is actually producing a probability between 0 and 1. The `predict` method simply returns 0 if this probability is less than .5 and 1 if it is greater.\n",
    "\n",
    "## View the underlying probabilities with `predict_proba`\n",
    "Instead of calling the `predict` method, call the `predict_proba` method to return the actual underlying probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logr.predict_proba(inputs)\n",
    "probs[:10].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does `predict_proba` return two columns? \n",
    "The `predict_proba` method returns a probability for each unique prediction class. Since we have two classes (No and Yes) it returns two columns.\n",
    "\n",
    "### The meaning of the first row\n",
    "The first row above shows its two values as .15 and .85. This means that the model is predicting a person with a `max_hr` of 71 to have a 15% chance of not having heart disease and an 85% chance of having it.\n",
    "\n",
    "### Visualizing the probability for all possible values of `max_hr`\n",
    "The scatterplot below shows the true outcome (0 or 1) for each value of `max_hr` in our actual dataset. Additionally, the estimated probabilities are drawn with a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(inputs, probs[:, 1])\n",
    "ax.scatter(X, y)\n",
    "ax.set_title('Probability of heart disease given max_heart_rate', fontsize=20);\n",
    "ax.set_xlabel('max_hr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isn't logistic regression supposed to draw an S curve?\n",
    "If you run a Google image search on \"logistic regression\", you will see lots of steeper curves with an appearance similar to the letter \"s\". The plot of probabilities above doesn't quite look like those images.\n",
    "\n",
    "### Use a wider range of data\n",
    "Notice at the end points of the graph above, our probabilities don't reach 0 or 1. If we use a wider range for `max_hr` - much further outside the range that is possible - and calculate the probabilities again, we will then get our s-curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-50, 350)\n",
    "x = x.reshape(-1, 1)\n",
    "probs = logr.predict_proba(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(x, probs[:, 1])\n",
    "ax.scatter(X, y)\n",
    "ax.set_xlabel('max_hr')\n",
    "ax.set_title('Probability of heart disease given max_heart_rate', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameter values of the logistic regression model\n",
    "With one feature, the logistic regression model is mathematically defined as the following equation:\n",
    "\n",
    "## $$ y = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)} }$$\n",
    "\n",
    "There are two parameters in this model. $\\beta_0$ is called the **bias** term (or intercept) and $\\beta_1$ is the **coefficient** corresponding to `max_hr`. For each additional features in the model, there would be an additional parameter.\n",
    "\n",
    "### Accessing the parameters from our object\n",
    "The bias and coefficient terms are found respectively in the `intercept_` and `coef_` attributes of `logr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the probabilities manually\n",
    "Let's define a function that calculates the probability from the logistic regression equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(x):\n",
    "    x1 = logr.intercept_ + logr.coef_ * x\n",
    "    return 1 / (1 + np.exp(-x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out our function\n",
    "Our function only returns the a single column array, the probability of having heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logr.predict_proba(inputs)\n",
    "my_probs = compute_probs(inputs)\n",
    "my_probs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the output is the same\n",
    "To verify that the output is the same, we can use the `allclose` NumPy function which returns a single boolean value if every value in the two arrays are within some small margin of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(probs[:, 1], my_probs[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "<span  style=\"color:green; font-size:16px\">Use the docstrings to help you complete the following function. Test your function with different inputs.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_learn_plot(df, columns, target):\n",
    "    \"\"\"\n",
    "    This function produces a single plot of the actual values of the target \n",
    "    variable along with the estimated probability curve from the logistic \n",
    "    regression model. It will be able to produce the plot above in this workbook, \n",
    "    but for several columns.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A Pandas DataFrame ready for machine learning\n",
    "    \n",
    "    columns: A list of string names of columns to use to \n",
    "        train the model.\n",
    "        \n",
    "    target: a single string of the target variable column name\n",
    "    \n",
    "    How to implement\n",
    "    ----------------\n",
    "    For every column in the `columns` list:\n",
    "        * Train a logistic regression model with that column as the only feature\n",
    "        * Use the `target` string column name as the target variable\n",
    "        * Use np.linspace to create an array of 100 inputs from min to max of that column\n",
    "        * Calculate a probability of being in each class for each input\n",
    "        * Make a scatterplot of the values of the column vs the true value of the output (0 or 1)\n",
    "        * Make a line plot of the inputs vs the calculated probability\n",
    "        * Make sure you are using a new Axes for every column\n",
    "        * Use a single Figure if you can\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with this\n",
    "log_reg_learn_plot(heart, ['max_hr', 'chol', 'old_peak', 'rest_bp'], 'disease')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
